# Exercise - Model Evaluation

### **When Evaluating Models, Remember:**  
> **"All models are wrong, some are useful."**  
> — *George Box* [(bio)](https://en.wikipedia.org/wiki/George_E._P._Box)  

### **Key Lessons:**  
- **Don't sweat initial model performance**. No model is perfect and early models are rarely even close. But they can still provide value and **improve with iteration**.   
- **Confidence scores** indicate how certain a model is about its predictions. Lower confidence may require further training and tuning.  
- Even imperfect models can assist in **data gathering, hypothesis testing, and guiding further research**.  
- **Human-in-the-loop** approaches can improve model performance by refining labels and reducing errors over time.  
- Models should be seen as **tools** to support decision-making, not as absolute truth.  
- Model predictions should **complement, not replace, expert human validation**.  
- Always evaluate a model’s **limitations, biases, and real-world applicability**. 

![](../01_module/slide_images/slide_60.png)

![](../01_module/slide_images/slide_62.png)

![](../01_module/slide_images/slide_61.png)

## Model-Maintenance | Read More: 
### [Model-Maintenance](https://www.ultralytics.com/blog/optimize-computer-vision-solutions-with-smart-model-maintenance)
### [Data Drift Machine Learning](https://spotintelligence.com/2024/04/08/data-drift-in-machine-learning/)